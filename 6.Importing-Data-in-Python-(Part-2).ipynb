{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing flat files from the web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('winequality-red.csv', <http.client.HTTPMessage at 0x7fb40cd090f0>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import pandas as pd\n",
    "\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "urlretrieve(url, 'winequality-red.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('winequality-red.csv', sep=';')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['1700', '1900'])\n",
      "                 country       1700\n",
      "0            Afghanistan  34.565000\n",
      "1  Akrotiri and Dhekelia  34.616667\n",
      "2                Albania  41.312000\n",
      "3                Algeria  36.720000\n",
      "4         American Samoa -14.307000\n"
     ]
    }
   ],
   "source": [
    "# read excel file from the web\n",
    "\n",
    "# Import package\n",
    "import pandas as pd\n",
    "\n",
    "# Assign url of file: url\n",
    "url = 'http://s3.amazonaws.com/assets.datacamp.com/course/importing_data_into_r/latitude.xls'\n",
    "\n",
    "# Read in all sheets of Excel file: xl\n",
    "xl = pd.read_excel(url, sheetname=None)\n",
    "\n",
    "# Print the sheetnames to the shell\n",
    "print(xl.keys())\n",
    "\n",
    "# Print the head of the first sheet (using its name, NOT its index)\n",
    "print(xl['1700'].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL\n",
    "* Uniform/Universal Resource Locator\n",
    "* References to web resources\n",
    "* Focus: web addresses\n",
    "* Ingredients:\n",
    "    * Protocol identifier - http:\n",
    "    * Resource name - datacamp.com\n",
    "* These specify web addresses uniquely\n",
    "* Going to a website = sending HTTP request\n",
    "    * GET request\n",
    "* urlretrieve() performs a GET request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlopen, Request\n",
    "url = \"https://www.wikipedia.org/\"\n",
    "request = Request(url)\n",
    "response = urlopen(request)\n",
    "html = response.read()\n",
    "response.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = \"https://www.wikipedia.org/\"\n",
    "r = requests.get(url)\n",
    "text = r.text\n",
    "#text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping the web in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url ='https://www.python.org/~guido/'\n",
    "r = requests.get(url)\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc,\"lxml\")\n",
    "#print(soup.prettify())\n",
    "print(soup.title)\n",
    "#print(soup.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "http://metalab.unc.edu/Dave/Dr-Fun/df200004/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "https://plus.google.com/u/0/115212051037621986145/posts\n",
      "http://www.dropbox.com\n",
      "Resume.html\n",
      "http://groups.google.com/groups?q=comp.lang.python\n",
      "http://stackoverflow.com\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a'):\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Guido's Personal Home Page</title>\n",
      "pics.html\n",
      "http://www.washingtonpost.com/wp-srv/business/longterm/microsoft/stories/1998/raymond120398.htm\n",
      "http://metalab.unc.edu/Dave/Dr-Fun/df200004/df20000406.jpg\n",
      "http://neopythonic.blogspot.com/2016/04/kings-day-speech.html\n",
      "http://www.python.org\n",
      "Resume.html\n",
      "Publications.html\n",
      "bio.html\n",
      "http://legacy.python.org/doc/essays/\n",
      "http://legacy.python.org/doc/essays/ppt/\n",
      "interviews.html\n",
      "pics.html\n",
      "http://neopythonic.blogspot.com\n",
      "http://www.artima.com/weblogs/index.jsp?blogger=12088\n",
      "https://twitter.com/gvanrossum\n",
      "https://plus.google.com/u/0/115212051037621986145/posts\n",
      "http://www.dropbox.com\n",
      "Resume.html\n",
      "http://groups.google.com/groups?q=comp.lang.python\n",
      "http://stackoverflow.com\n",
      "guido.au\n",
      "http://legacy.python.org/doc/essays/\n",
      "images/license.jpg\n",
      "http://www.cnpbagwell.com/audio-faq\n",
      "http://sox.sourceforge.net/\n",
      "images/internetdog.gif\n"
     ]
    }
   ],
   "source": [
    "# Specify url\n",
    "url = 'https://www.python.org/~guido/'\n",
    "\n",
    "# Package the request, send the request and catch the response: r\n",
    "r = requests.get(url)\n",
    "\n",
    "# Extracts the response as html: html_doc\n",
    "html_doc = r.text\n",
    "\n",
    "# create a BeautifulSoup object from the HTML: soup\n",
    "soup = BeautifulSoup(html_doc,\"lxml\")\n",
    "\n",
    "# Print the title of Guido's webpage\n",
    "print(soup.title)\n",
    "\n",
    "# Find all 'a' tags (which define hyperlinks): a_tags\n",
    "a_tags = soup.find_all('a')\n",
    "\n",
    "# Print the URLs to the shell\n",
    "for link in a_tags:\n",
    "    print(link.get('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
